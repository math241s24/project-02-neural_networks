---
title: "Neural Networks"
subtitle: "Project 2"
author: "Elliott Chang, Cameron Adams"
title-slide-attributes:
  data-slide-number: none
format: revealjs
editor: visual
execute:
  echo: false
---

```{r}
#| label: load-packages
#| include: false

library(tidyverse)
```

```{r}
#| label: setup
#| include: false

# For better figure resolution
knitr::opts_chunk$set(
  fig.retina = 3, 
  dpi = 300, 
  fig.width = 6, 
  fig.asp = 0.618, 
  out.width = "70%"
  )
```

```{r}
#| label: load-data
#| include: false

genGauss <- function(cx, cy, num_samples, variance, label) {
  x <- rnorm(num_samples, mean=cx, sd=sqrt(variance))
  y <- rnorm(num_samples, mean=cy, sd=sqrt(variance))
  data.frame(x=x, y=y, label=label)
}

classifyTwoGaussData <- function(num_samples, noise) {
  variance_scale <- function(noise) { (1 - noise) * 0.5 + noise * 4 }
  variance <- variance_scale(noise)
  points1 <- genGauss(2, 2, num_samples/2, variance, 'b')
  points2 <- genGauss(-2, -2, num_samples/2, variance, 'a')
  points <- rbind(points1, points2)
  return(points)
}

set.seed(123)
data_two <- classifyTwoGaussData(1000, 0.1)
gauss_plot <- ggplot(data_two, aes(x=x, y=y, color=factor(label))) +
  geom_point(alpha=0.6) +
  labs(title="Classify Two Gauss Data", color="Label") +
  theme_minimal()


generateDataByX <- function(num_samples) {
  x <- runif(num_samples, min=-5, max=5)
  y <- runif(num_samples, min=-5, max=5)
  label <- ifelse(x >= 0, 'a', 'b')
  data <- data.frame(x=x, y=y, label=label)
  return(data)
}

set.seed(123)
data_by_x <- generateDataByX(1000)
x_plot <- ggplot(data_by_x, aes(x=x, y=y, color=factor(label))) +
  geom_point(alpha=0.6) +
  labs(title="Data Classified by X Value", color="Label") +
  theme_minimal()


generateDataByXYProduct <- function(num_samples) {
  x <- runif(num_samples, min=-5, max=5)
  y <- runif(num_samples, min=-5, max=5)
  label <- ifelse(x * y >= 0, 'b', 'a')
  data <- data.frame(x=x, y=y, label=label)
  return(data)
}

set.seed(123)
data_by_xy <- generateDataByXYProduct(1000)
xy_plot <- ggplot(data_by_xy, aes(x=x, y=y, color=factor(label))) +
  geom_point(alpha=0.6) +
  labs(title="Data Classified by XY Product", color="Label") +
  theme_minimal()


generateSpiralData <- function(num_samples, noise) {
  n <- num_samples / 2  # Half for each label
  points <- data.frame(x=numeric(0), y=numeric(0), label=character(0))
  genSpiral <- function(deltaT, label) {
    for (i in seq_len(n)) {
      r <- i / n * 5
      t <- 1.75 * i / n * 2 * pi + deltaT
      x <- r * sin(t) + rnorm(1, mean=0, sd=noise)
      y <- r * cos(t) + rnorm(1, mean=0, sd=noise)
      points <<- rbind(points, data.frame(x=x, y=y, label=label))
    }
  }
  genSpiral(0, 'b')  # Generating points labeled 'b'
  genSpiral(pi, 'a')  # Generating points labeled 'a'
  return(points)
}

set.seed(123)
spiral_data <- generateSpiralData(1000, 0.1)
spiral_plot <- ggplot(spiral_data, aes(x=x, y=y, color=factor(label))) +
  geom_point(alpha=0.6) +
  labs(title="Spiral Data", color="Label") +
  theme_minimal()
```

## Introduction

We plan on illustrating the predictive power and flexibility of Neural Networks on data with complex relationships.

-   For the sake of this project, we decided to focus on classification

-   

## The data:

To illustrate the power and flexibility of neural networks, we created multiple datasets with complex relationships.

-   Each dataset contains two continuous predictor variables `x` and `y`, and one categorical response variable `label` with factors `a` and `b`.

-   Two coninuous predictors allow us to visualize the relationships between these predictors easily, giving us an intuitive understand of what the underlying relationship might be

## The Data (Continued)

```{r}
library(cowplot)

plot_grid(gauss_plot, x_plot, xy_plot, spiral_plot)

```

-   Despite the intuitive nature of the relationships, modeling them is quite difficult using tools that we've learned about so far in this class

## What is a Neural Network?

![Image](data/neuralnetwork.png)

## Training Neural Networks

Training neural network involves

-   Training/testing split

-   Specifying the architecture of your neural network

    -   How many layers?

    -   How many nodes?

    -   Training rate?

    -   Epochs?

## Testing Neural Networks

Testing a neural network in the context of classification tasks involves

-   Use the model to make predictions on the test data

-   Compare the predictions to the true values

    -   Overall accuracy?

    -   Specificity?

    -   Sensitivity?

## Using our data...

Want to add code chunks with tabs

```{r}
#| echo: false
train_test_split <- function(data) {
  data_rows <- floor(0.80 * nrow(data))
  train_indices <- sample(1:nrow(data), data_rows)
  train_data <- data[train_indices, ]
  test_data <- data[-train_indices, ]
  return(list(train=train_data, test=test_data))
}

set.seed(2)
split_data1 <- train_test_split(data_two)
train1 <- split_data1$train
test1 <- split_data1$test
split_data2 <- train_test_split(data_by_x)
train2 <- split_data2$train
test2 <- split_data2$test
split_data3 <- train_test_split(data_by_xy)
train3 <- split_data3$train
test3 <- split_data3$test
split_data4 <- train_test_split(spiral_data)
train4 <- split_data4$train
test4 <- split_data4$test
```

::: panel-tabset
## bye

```{r}
#| label: confusion-matrix
#| warning: false
#| message: false

library(neuralnet)
model4 <- neuralnet(
    label~x + y,
    data=train4,
    hidden=c(8,8),
    linear.output=FALSE,
    learningrate=0.001,
    stepmax=1e+06
)

pred <- predict(model4, test4)
labels <- c("a", "b")
prediction_label <- data.frame(max.col(pred)) %>%
  mutate(pred=labels[max.col(pred)]) %>%
  select(2) %>%
  unlist()
table(test4$label, prediction_label)
```

## hi

```{r}
#| label: defnw
#| echo: false
table(test4$label, prediction_label)
```
:::

## Conclusion

-   Neural Networks are extremely powerful, especially in the context of classification

-   Neural Networks are significantly more flexible than other algorithms
