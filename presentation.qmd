---
title: "Neural Networks"
subtitle: "Project 2"
author: "Elliott Chang, Cameron Adams"
title-slide-attributes:
  data-slide-number: none
format:
  revealjs:
    transition: slide
    incremental: true 
editor: visual
execute:
  echo: false
---

```{r}
#| label: load-packages
#| include: false

library(tidyverse)
```

```{r}
#| label: setup
#| include: false

# For better figure resolution
knitr::opts_chunk$set(
  fig.retina = 3, 
  dpi = 300, 
  fig.width = 6, 
  fig.asp = 0.618, 
  out.width = "70%"
  )
```

```{r}
#| label: load-data
#| include: false

genGauss <- function(cx, cy, num_samples, variance, label) {
  x <- rnorm(num_samples, mean=cx, sd=sqrt(variance))
  y <- rnorm(num_samples, mean=cy, sd=sqrt(variance))
  data.frame(x=x, y=y, label=label)
}

classifyTwoGaussData <- function(num_samples, noise) {
  variance_scale <- function(noise) { (1 - noise) * 0.5 + noise * 4 }
  variance <- variance_scale(noise)
  points1 <- genGauss(2, 2, num_samples/2, variance, 'b')
  points2 <- genGauss(-2, -2, num_samples/2, variance, 'a')
  points <- rbind(points1, points2)
  return(points)
}

set.seed(123)
data_two <- classifyTwoGaussData(1000, 0.1)
gauss_plot <- ggplot(data_two, aes(x=x, y=y, color=factor(label))) +
  geom_point(alpha=0.6) +
  labs(title="Classify Two Gauss Data", color="Label") +
  theme_minimal()


generateDataByX <- function(num_samples) {
  x <- runif(num_samples, min=-5, max=5)
  y <- runif(num_samples, min=-5, max=5)
  label <- ifelse(x >= 0, 'a', 'b')
  data <- data.frame(x=x, y=y, label=label)
  return(data)
}

set.seed(123)
data_by_x <- generateDataByX(1000)
x_plot <- ggplot(data_by_x, aes(x=x, y=y, color=factor(label))) +
  geom_point(alpha=0.6) +
  labs(title="Data Classified by X Value", color="Label") +
  theme_minimal()


generateDataByXYProduct <- function(num_samples) {
  x <- runif(num_samples, min=-5, max=5)
  y <- runif(num_samples, min=-5, max=5)
  label <- ifelse(x * y >= 0, 'b', 'a')
  data <- data.frame(x=x, y=y, label=label)
  return(data)
}

set.seed(123)
data_by_xy <- generateDataByXYProduct(1000)
xy_plot <- ggplot(data_by_xy, aes(x=x, y=y, color=factor(label))) +
  geom_point(alpha=0.6) +
  labs(title="Data Classified by XY Product", color="Label") +
  theme_minimal()


generateSpiralData <- function(num_samples, noise) {
  n <- num_samples / 2  # Half for each label
  points <- data.frame(x=numeric(0), y=numeric(0), label=character(0))
  genSpiral <- function(deltaT, label) {
    for (i in seq_len(n)) {
      r <- i / n * 5
      t <- 1.75 * i / n * 2 * pi + deltaT
      x <- r * sin(t) + rnorm(1, mean=0, sd=noise)
      y <- r * cos(t) + rnorm(1, mean=0, sd=noise)
      points <<- rbind(points, data.frame(x=x, y=y, label=label))
    }
  }
  genSpiral(0, 'b')  # Generating points labeled 'b'
  genSpiral(pi, 'a')  # Generating points labeled 'a'
  return(points)
}

set.seed(123)
spiral_data <- generateSpiralData(1000, 0.1)
spiral_plot <- ggplot(spiral_data, aes(x=x, y=y, color=factor(label))) +
  geom_point(alpha=0.6) +
  labs(title="Spiral Data", color="Label") +
  theme_minimal()
```

## Introduction

We plan on illustrating the predictive power and flexibility of Neural Networks on data with complex relationships.

-   We decided to focus on classification tasks, with applications in clustering and computer vision.

Research Question: 

-   How accurately can a simple neural network classify points with their correct clusters?

-   What are the benefits of using a neural network over other algorithms?

## The Data

To illustrate the power and flexibility of neural networks, we created multiple datasets with complex relationships.

-   Each dataset contains two continuous predictor variables `x` and `y`, and one categorical response variable `label` with factors `a` and `b`.

-   Two coninuous predictors allow us to visualize the relationships between these predictors easily, giving us an intuitive understand of what the underlying relationship might be

## The Data

```{r}
library(cowplot)

plot_grid(gauss_plot, x_plot, xy_plot, spiral_plot)

```

-   Despite the intuitive nature of the relationships, modeling them is quite difficult using tools that we've learned about so far in this class


## What is a Neural Network? {.smaller}

-   a machine learning algorithm modeled on the human brain and nervous system (hence NEURAL network)

-   contains a network of nodes, organized into layers, each of which can be thought of as its own linear regression model
    -   analogously, the 'weights' associated to a node can be viewed as slope coefficients and 'biases' can be viewed as the intercept terms
    -   weights and biases are assigned randomly, then iteratively changed to reduce training error

-   each additional node allows for a different linear relationships... adding more 'layers' of nodes allows for more flexible, non-linear relationships

-   similar to decision trees, one can think of neural networks as 'partitioning' the predictor space

## What is a Neural Network?

![](data/neuralnetwork.png)


## Training Neural Networks

Training neural network involves

-   Training/testing split

-   Specifying the architecture of your neural network

    -   How many layers?

    -   How many nodes?

    -   Training rate?

    -   Epochs?

## Testing Neural Networks

Testing a neural network in the context of classification tasks involves

-   Use the model to make predictions on the test data

-   Compare the predictions to the true values

    -   Overall accuracy?

    -   Specificity?

    -   Sensitivity?
    
    
```{r}
generate_grid <- function(data) {
  x_range <- seq(min(data$x)-1, max(data$x)+1, length.out=200)
  y_range <- seq(min(data$y)-1, max(data$y)+1, length.out=200)
  grid <- expand.grid(x=x_range, y=y_range)
  return(grid)
}

grid1 <- generate_grid(data_two)
grid2 <- generate_grid(data_by_x)
grid3 <- generate_grid(data_by_xy)
grid4 <- generate_grid(spiral_data)

predict_grid <- function(model, grid) {
  grid$pred <- predict(model, grid)
  grid$pred_label <- ifelse(grid$pred[,1] > grid$pred[,2], 'a', 'b')
  return(grid)
}
```


## Gauss Model

```{r}
#| echo: false
train_test_split <- function(data) {
  data_rows <- floor(0.80 * nrow(data))
  train_indices <- sample(1:nrow(data), data_rows)
  train_data <- data[train_indices, ]
  test_data <- data[-train_indices, ]
  return(list(train=train_data, test=test_data))
}

set.seed(2)
split_data1 <- train_test_split(data_two)
train1 <- split_data1$train
test1 <- split_data1$test
split_data2 <- train_test_split(data_by_x)
train2 <- split_data2$train
test2 <- split_data2$test
split_data3 <- train_test_split(data_by_xy)
train3 <- split_data3$train
test3 <- split_data3$test
split_data4 <- train_test_split(spiral_data)
train4 <- split_data4$train
test4 <- split_data4$test

plot_prediction <- function(grid, data, title) {
  ggplot() +
    geom_tile(data=grid, aes(x=x, y=y, fill=pred_label), alpha=0.5) +
    geom_point(data=data, aes(x=x, y=y, color=label), alpha=0.8) +
    scale_fill_manual(values=c('a'='blue', 'b'='red')) +
    scale_color_manual(values=c('a'='blue', 'b'='red')) +
    labs(title=title, fill="Predicted Label", color="Actual Label") +
    theme_minimal()
}
```

::: panel-tabset
<<<<<<< HEAD
## bye

=======
## Code
>>>>>>> 8aa147f4a71c93d901697e2824f639a4e31164b5
```{r}
#| label: model1
#| warning: false
#| message: false
#| include: true
#| echo: true

library(neuralnet)
model1 <- neuralnet(
  label~x+y, 
  data=train1, 
  hidden=c(5,5), 
  linear.output=FALSE)
```

```{r}
#| echo: false

pred <- predict(model1, test1)
labels <- c("a", "b")
prediction_label <- data.frame(max.col(pred)) %>%
  mutate(pred=labels[max.col(pred)]) %>%
  select(2) %>%
  unlist()
table(test1$label, prediction_label)

check <- ifelse(test1$label == 'a', 1, 2) == max.col(pred)
accuracy <- (sum(check)/nrow(test1))*100
data_frame(accuracy = accuracy)
```


## Architecture

```{r}
#| label: arc1
#| echo: false

plot(model1, rep="best")
```


## Predictions

```{r}
#| echo: false


grid1$pred_label <- predict_grid(model1, grid1)$pred_label
```

```{r}
#| label: plot1
#| echo: false

plot_prediction(grid1, data_two, "Model 1 Predictions")
```
:::

## X Model

::: panel-tabset
## Code
```{r}
#| label: model2
#| warning: false
#| message: false
#| include: true
#| echo: true

library(neuralnet)
model2 <- neuralnet(
  label~x+y, 
  data=train2, 
  hidden=c(5,5), 
  linear.output=FALSE)
```

```{r}
#| echo: false

pred <- predict(model2, test2)
labels <- c("a", "b")
prediction_label <- data.frame(max.col(pred)) %>%
  mutate(pred=labels[max.col(pred)]) %>%
  select(2) %>%
  unlist()
table(test2$label, prediction_label)

check <- ifelse(test2$label == 'a', 1, 2) == max.col(pred)
accuracy <- (sum(check)/nrow(test2))*100
data_frame(accuracy = accuracy)
```


## Architecture

```{r}
#| label: arc2
#| echo: false

plot(model2, rep="best")
```


## Predictions

```{r}
#| echo: false


grid2$pred_label <- predict_grid(model2, grid2)$pred_label
```

```{r}
#| label: plot2
#| echo: false

plot_prediction(grid2, data_by_x, "Model 2 Predictions")
```
:::

## XY Model

::: panel-tabset
## Code
```{r}
#| label: model3
#| warning: false
#| message: false
#| include: true
#| echo: true

library(neuralnet)
model3 <- neuralnet(
  label~x+y, 
  data=train3, 
  hidden=c(5,5), 
  linear.output=FALSE)
```

```{r}
#| echo: false

pred <- predict(model3, test3)
labels <- c("a", "b")
prediction_label <- data.frame(max.col(pred)) %>%
  mutate(pred=labels[max.col(pred)]) %>%
  select(2) %>%
  unlist()
table(test3$label, prediction_label)

check <- ifelse(test3$label == 'a', 1, 2) == max.col(pred)
accuracy <- (sum(check)/nrow(test3))*100
data_frame(accuracy = accuracy)
```


## Architecture

```{r}
#| label: arc3
#| echo: false

plot(model3, rep="best")
```


## Predictions

```{r}
#| echo: false


grid3$pred_label <- predict_grid(model3, grid3)$pred_label
```

```{r}
#| label: plot3
#| echo: false

plot_prediction(grid3, data_by_xy, "Model 3 Predictions")
```
:::

## Spiral Model
::: panel-tabset

## Code
```{r}
#| label: model4
#| warning: false
#| message: false
#| include: true
#| echo: true

library(neuralnet)
model4 <- neuralnet(
    label~x + y,
    data=train4, 
    hidden=c(8,8), #two hidden layers with 8 nodes each
    linear.output=FALSE,
    learningrate=0.001,
    stepmax=1e+06
)
```

```{r}
#| echo: false

pred <- predict(model4, test4)
labels <- c("a", "b")
prediction_label <- data.frame(max.col(pred)) %>%
  mutate(pred=labels[max.col(pred)]) %>%
  select(2) %>%
  unlist()
table(test4$label, prediction_label)

check <- ifelse(test4$label == 'a', 1, 2) == max.col(pred)
accuracy <- (sum(check)/nrow(test4))*100
data_frame(accuracy = accuracy)
```

<<<<<<< HEAD
## hi
=======

## Predictions
>>>>>>> 8aa147f4a71c93d901697e2824f639a4e31164b5

```{r}
#| echo: false


grid4$pred_label <- predict_grid(model4, grid4)$pred_label
```

```{r}
#| label: plot
#| echo: false

plot_prediction(grid4, spiral_data, "Model 4 Predictions")
```
:::

## Conclusion

-   Neural Networks are extremely powerful, especially in the context of classification

-   Neural Networks are significantly more flexible than other algorithms
