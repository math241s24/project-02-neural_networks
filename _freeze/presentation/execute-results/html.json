{
  "hash": "26fce258fca325a27c0a80376ef378f2",
  "result": {
    "markdown": "---\ntitle: \"Neural Networks\"\nsubtitle: \"Project 2\"\nauthor: \"Elliott Chang, Cameron Adams\"\ntitle-slide-attributes:\n  data-slide-number: none\nformat:\n  revealjs:\n    transition: slide\n    incremental: true \neditor: visual\nexecute:\n  echo: false\n---\n\n\n\n\n\n\n\n\n## Introduction\n\nWe plan on illustrating the predictive power and flexibility of Neural Networks on data with complex relationships.\n\n-   We decided to focus on classification tasks, with applications in clustering and computer vision.\n\nResearch Question: \n\n-   How accurately can a simple neural network classify points with their correct clusters?\n\n-   What are the benefits of using a neural network over other algorithms?\n\n## The Data\n\nTo illustrate the power and flexibility of neural networks, we created multiple datasets with complex relationships.\n\n-   Each dataset contains two continuous predictor variables `x` and `y`, and one categorical response variable `label` with factors `a` and `b`.\n\n-   Two coninuous predictors allow us to visualize the relationships between these predictors easily, giving us an intuitive understand of what the underlying relationship might be\n\n## The Data\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](presentation_files/figure-revealjs/unnamed-chunk-4-1.png){width=70%}\n:::\n:::\n\n\n-   Despite the intuitive nature of the relationships, modeling them is quite difficult using tools that we've learned about so far in this class\n\n\n## What is a Neural Network? {.smaller}\n\n-   a machine learning algorithm modeled on the human brain and nervous system (hence NEURAL network)\n\n-   contains a network of nodes, organized into layers, each of which can be thought of as its own linear regression model\n    -   analogously, the 'weights' associated to a node can be viewed as slope coefficients and 'biases' can be viewed as the intercept terms\n    -   weights and biases are assigned randomly, then iteratively changed to reduce training error\n\n-   each additional node allows for a different linear relationships... adding more 'layers' of nodes allows for more flexible, non-linear relationships\n\n-   similar to decision trees, one can think of neural networks as 'partitioning' the predictor space\n\n## What is a Neural Network?\n\n![](data/neuralnetwork.png)\n\n\n## Training Neural Networks\n\nTraining neural network involves\n\n-   Training/testing split\n\n-   Specifying the architecture of your neural network\n\n    -   How many layers?\n\n    -   How many nodes?\n\n    -   Training rate?\n\n    -   Epochs?\n\n## Testing Neural Networks\n\nTesting a neural network in the context of classification tasks involves\n\n-   Use the model to make predictions on the test data\n\n-   Compare the predictions to the true values\n\n    -   Overall accuracy?\n\n    -   Specificity?\n\n    -   Sensitivity?\n    \n    \n\n::: {.cell}\n\n:::\n\n\n\n## Gauss Model\n\n\n::: {.cell}\n\n:::\n\n\n::: panel-tabset\n## Code\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(neuralnet)\nmodel1 <- neuralnet(\n  label~x+y, \n  data=train1, \n  hidden=c(5,5), \n  linear.output=FALSE)\n```\n:::\n\n::: {.cell}\n::: {.cell-output .cell-output-stdout}\n```\n   prediction_label\n      a   b\n  a 113   0\n  b   0  87\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 1 × 1\n  accuracy\n     <dbl>\n1      100\n```\n:::\n:::\n\n\n\n## Architecture\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](presentation_files/figure-revealjs/arc1-1.png){width=70%}\n:::\n:::\n\n\n\n## Predictions\n\n\n::: {.cell}\n\n:::\n\n::: {.cell}\n::: {.cell-output-display}\n![](presentation_files/figure-revealjs/plot1-1.png){width=70%}\n:::\n:::\n\n:::\n\n## X Model\n\n::: panel-tabset\n## Code\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(neuralnet)\nmodel2 <- neuralnet(\n  label~x+y, \n  data=train2, \n  hidden=c(5,5), \n  linear.output=FALSE)\n```\n:::\n\n::: {.cell}\n::: {.cell-output .cell-output-stdout}\n```\n   prediction_label\n      a   b\n  a  99   0\n  b   0 101\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 1 × 1\n  accuracy\n     <dbl>\n1      100\n```\n:::\n:::\n\n\n\n## Architecture\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](presentation_files/figure-revealjs/arc2-1.png){width=70%}\n:::\n:::\n\n\n\n## Predictions\n\n\n::: {.cell}\n\n:::\n\n::: {.cell}\n::: {.cell-output-display}\n![](presentation_files/figure-revealjs/plot2-1.png){width=70%}\n:::\n:::\n\n:::\n\n## XY Model\n\n::: panel-tabset\n## Code\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(neuralnet)\nmodel3 <- neuralnet(\n  label~x+y, \n  data=train3, \n  hidden=c(5,5), \n  linear.output=FALSE)\n```\n:::\n\n::: {.cell}\n::: {.cell-output .cell-output-stdout}\n```\n   prediction_label\n      a   b\n  a 104   1\n  b   3  92\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 1 × 1\n  accuracy\n     <dbl>\n1       98\n```\n:::\n:::\n\n\n\n## Architecture\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](presentation_files/figure-revealjs/arc3-1.png){width=70%}\n:::\n:::\n\n\n\n## Predictions\n\n\n::: {.cell}\n\n:::\n\n::: {.cell}\n::: {.cell-output-display}\n![](presentation_files/figure-revealjs/plot3-1.png){width=70%}\n:::\n:::\n\n:::\n\n## Spiral Model\n::: panel-tabset\n\n## Code\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(neuralnet)\nmodel4 <- neuralnet(\n    label~x + y,\n    data=train4, \n    hidden=c(8,8), #two hidden layers with 8 nodes each\n    linear.output=FALSE,\n    learningrate=0.001,\n    stepmax=1e+06\n)\n```\n:::\n\n::: {.cell}\n::: {.cell-output .cell-output-stdout}\n```\n   prediction_label\n     a  b\n  a 94  3\n  b  8 95\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 1 × 1\n  accuracy\n     <dbl>\n1     94.5\n```\n:::\n:::\n\n\n\n## Predictions\n\n\n::: {.cell}\n\n:::\n\n::: {.cell}\n::: {.cell-output-display}\n![](presentation_files/figure-revealjs/plot-1.png){width=70%}\n:::\n:::\n\n\n\n:::\n\n## Conclusion\n\n-   Neural Networks are extremely powerful, especially in the context of classification\n\n-   Neural Networks are significantly more flexible than other algorithms\n",
    "supporting": [
      "presentation_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {
      "include-after-body": [
        "\n<script>\n  // htmlwidgets need to know to resize themselves when slides are shown/hidden.\n  // Fire the \"slideenter\" event (handled by htmlwidgets.js) when the current\n  // slide changes (different for each slide format).\n  (function () {\n    // dispatch for htmlwidgets\n    function fireSlideEnter() {\n      const event = window.document.createEvent(\"Event\");\n      event.initEvent(\"slideenter\", true, true);\n      window.document.dispatchEvent(event);\n    }\n\n    function fireSlideChanged(previousSlide, currentSlide) {\n      fireSlideEnter();\n\n      // dispatch for shiny\n      if (window.jQuery) {\n        if (previousSlide) {\n          window.jQuery(previousSlide).trigger(\"hidden\");\n        }\n        if (currentSlide) {\n          window.jQuery(currentSlide).trigger(\"shown\");\n        }\n      }\n    }\n\n    // hookup for slidy\n    if (window.w3c_slidy) {\n      window.w3c_slidy.add_observer(function (slide_num) {\n        // slide_num starts at position 1\n        fireSlideChanged(null, w3c_slidy.slides[slide_num - 1]);\n      });\n    }\n\n  })();\n</script>\n\n"
      ]
    },
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}